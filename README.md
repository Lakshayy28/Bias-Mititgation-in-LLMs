# Bias-Mititgation-in-LLMs
Mitigating the language bias across Toxicity, Honest and Regard metrics. A comparative  benchmarking of bias metrics for GPT2, LLama2 and Falcon LLMs

Implementation of Counterfactual data substitution to reduce language polarity in GPT2. 

We have studied different toxicity metrics namely: toxicity from hugging face evaluate library, Perspective API toxicity and Unitary detoxify API
Regard and honest metrics are part of the hugging face evaluate library.

Code will be open-sourced after completion of the project.
