# Bias-Mititgation-in-LLMs
Mitigating the language bias across Toxicity, Honest and Regard metrics. A comparative  benchmarking of bias metrics for GPT2, LLama2 and Falcon LLMs

Implementation of Counterfactual data substitution to reduce language polarity in GPT2. 

We have studied different toxicity metrics namely: toxicity from hugging face evaluate library, Perspective API toxicity and Unitary detoxify API
Regard and honest metrics are part of the hugging face evaluate library.

Code will be open-sourced after completion of the project.

Research papers: 
1. **Unveiling Toxic Tendencies of Small Language Models in Unconstrained Generation Tasks** 
   https://www.researchgate.net/publication/384205373_Unveiling_Toxic_Tendencies_of_Small_Language_Models_in_Unconstrained_Generation_Tasks

2. **A Study of Social Biases in Small Language Models and Mitigation Strategies**
   https://www.researchgate.net/publication/385536118_A_Study_of_Social_Biases_in_Small_Language_Models_and_Mitigation_Strategies
   

